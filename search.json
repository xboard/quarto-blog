[
  {
    "objectID": "posts/2023_07_27_tip_set_dependsonpast_in_airflow_when_creating_time_series_model_pipeline/index.html",
    "href": "posts/2023_07_27_tip_set_dependsonpast_in_airflow_when_creating_time_series_model_pipeline/index.html",
    "title": "Tip: set depends_on_past=True in Airflow when creating a time series model pipeline",
    "section": "",
    "text": "Tip\n\n\n\nWhen creating time series model pipeline in Airflow set depends_on_past=True."
  },
  {
    "objectID": "posts/2023_07_27_tip_set_dependsonpast_in_airflow_when_creating_time_series_model_pipeline/index.html#why",
    "href": "posts/2023_07_27_tip_set_dependsonpast_in_airflow_when_creating_time_series_model_pipeline/index.html#why",
    "title": "Tip: set depends_on_past=True in Airflow when creating a time series model pipeline",
    "section": "Why?",
    "text": "Why?\nTime series models help us predict future data based on patterns and trends observed in historical data. These models surprise, surprise‚Ä¶ depend on past data, hence any deviation from the expected sequence can affect their accuracy.\nOne way to ensure the integrity of the sequence of data while creating time series models in Apache Airflow is to set the depends_on_past argument to ‚ÄòTrue‚Äô. This way each subsequent task in the DAG will only run once its preceding task has completed successfully. This approach guarantees that the time series model is fed with the correct inputs in the correct order and that the historical data is correctly sequenced.\nIf you don‚Äôt set depends_on_past=True and you have a problem with one of the partitions of the data used by your model, one of two things will happen in the following days:\n\nWorst case scenario: your workflow will run without any issues being reported. This is a sign that you forgot to add sensors to test the availability of historical data and, chances are, your features are using incomplete data, to say the least.\nAlternatively, subsequent executions of your workflow will also fail, bombarding your oncall colleague‚Äôs pagerduty with messages. And when the problem is finally fixed, you still have to remember to clear and restart all the previous failed executions."
  },
  {
    "objectID": "posts/2023_07_27_tip_set_dependsonpast_in_airflow_when_creating_time_series_model_pipeline/index.html#how",
    "href": "posts/2023_07_27_tip_set_dependsonpast_in_airflow_when_creating_time_series_model_pipeline/index.html#how",
    "title": "Tip: set depends_on_past=True in Airflow when creating a time series model pipeline",
    "section": "How?",
    "text": "How?\nFor a specific task, most probably a sensor:\ntask = SomeSensor(\n    task_id='task',\n    depends_on_past=True,\n    # ...\n)\nor in your dag default_args to enable depends_on_past for all your tasks\ndefault_args = {\n    'depends_on_past': True,\n    # ...\n}\n\ndag = DAG(\n    dag_id=\"time_series_pipeline\",\n    default_args=default_args,\n    # ...\n)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "xboard.dev",
    "section": "",
    "text": "Tip: set depends_on_past=True in Airflow when creating a time series model pipeline\n\n\n\n\n\n\n\nairflow\n\n\ndata engineering\n\n\n\n\n\n\n\n\n\n\n\nJul 27, 2023\n\n\nFlavio Regis de Arruda\n\n\n\n\n\n\n  \n\n\n\n\nUsing pip-tools to manage project dependencies in Python\n\n\n\n\n\n\n\nsoftware development\n\n\ntools\n\n\npython\n\n\n\n\nHow to ensure you have a reproducible development environment.\n\n\n\n\n\n\nJul 20, 2023\n\n\nFlavio Regis de Arruda\n\n\n\n\n\n\n  \n\n\n\n\nInterrupted Time Series (ITS) in Python\n\n\n\n\n\n\n\ndata science\n\n\ncausal inference\n\n\npython\n\n\n\n\nInterrupted Time Series (ITS) analysis using Python and statsmodels.\n\n\n\n\n\n\nJan 1, 2022\n\n\nFlavio Regis de Arruda\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/2020_01_01_interrupted-time-series-python-part-I/index.html#when-ab-test-is-not-an-option",
    "href": "posts/2020_01_01_interrupted-time-series-python-part-I/index.html#when-ab-test-is-not-an-option",
    "title": "Interrupted Time Series (ITS) in Python",
    "section": "When A/B test is not an option",
    "text": "When A/B test is not an option\nThe gold standard for statistically asserting the effectiveness of an intervention is the randomized controlled experiment and its simplified online variant: the A/B test.\n\nüìù During an A/B test there are two almost identical versions of a product, simultaneously running, that only differ by the hypothesis you want to test ( i.e can a red call to action button convert more than a blue one? ). Users are randomly chosen to experience one (and only one) of the two versions while the experiment is active.\n\nThey are easy to understand, easy to setup (great free tools easily available) and when correctly designed they rule out any covariate differences between the groups.\nHowever, sometimes it‚Äôs just not possible to set up an A/B test:\n\nTechnical difficulties. Sometimes a change is so widespread and complex that it would be technically impossible to keep two different versions running simultaneously.\nBusiness strategy. A new feature rollout will be available first to some countries and later for others.\nEthical concerns. Having a subset of customers having access to a feature or bug fix that gives them a competitive advantage over others that don‚Äôt.\nLegal or regulatory requirements. A change in regulations becomes mandatory ( i.e.¬†GPDR compliance ) and should be applied to all your customers of a given country at the same time.\nTemporal infeasibility. You want to analyze an event that has already happened ( i.e.¬†How last Google‚Äôs search algorithm update impacted your sales funnel? )."
  },
  {
    "objectID": "posts/2020_01_01_interrupted-time-series-python-part-I/index.html#quasi-experiments",
    "href": "posts/2020_01_01_interrupted-time-series-python-part-I/index.html#quasi-experiments",
    "title": "Interrupted Time Series (ITS) in Python",
    "section": "Quasi Experiments",
    "text": "Quasi Experiments\n   \nIf you can‚Äôt do an A/B test then the second to best alternative are quasi experiments [1].\nIn a quasi experiment, your treatment and control group are not divided by a completely random process but by a natural process (i.e.¬†time, location, etc) therefore there is a much larger chance for imbalance due to skewness and heterogeneous differences. The results of a quasi-experiment won‚Äôt be as precise as an A/B, but if carefully conducted could be considered close enough to compute estimates.\nThere are some scenarios, like some described in the previous section, where having a control group in parallel to a test group is just not possible, and this is when Interrupted Times Series comes in very handy."
  },
  {
    "objectID": "posts/2020_01_01_interrupted-time-series-python-part-I/index.html#interrupted-time-series-its",
    "href": "posts/2020_01_01_interrupted-time-series-python-part-I/index.html#interrupted-time-series-its",
    "title": "Interrupted Time Series (ITS) in Python",
    "section": "Interrupted Time Series (ITS)",
    "text": "Interrupted Time Series (ITS)\nInterrupted time series (ITS) is a method of statistical analysis involving tracking a period before and after a intervention at a known point in time to assess the intervention‚Äôs effects within a single group/population. The time series refers to the data over the period, while the interruption is the intervention, which is a controlled external influence or set of influences. Effects of the intervention are evaluated by changes in the level and slope of the time series and statistical significance of the intervention parameters [2]. The more observations you have before and after the intervention, the more robust your model will be (typically). Because the evaluation is based on observing a single population over time, the ITS design is free from problems due to between-group difference but are susceptible to time-varying confounders like other interventions occurring around the time of the intervention that may also affect the outcome [3].\n  \n\nüëç Strengths of Interrupted Time Series include the ability to control for secular trends in the data (unlike a 2-period before-and-after \\(t\\)-test), ability to evaluate outcomes using population-level data, clear graphical presentation of results, ease of conducting stratified analyses, and ability to evaluate both intended and unintended consequences of interventions.\nüëé Limitations of Interrupted Time Series include the need for a minimum of 8 time periods before and 8 after an intervention to evaluate changes statistically, difficulty in analyzing the independent impact of separate components of a program that are implemented close together in time, and existence of a suitable control population.\n\nIn mathematical terms, it means that the time series equation \\((1)\\) includes four key coefficients:\n \\[Y = b_0 + b_1T + b_2D + b_3P + \\epsilon\\] \nWhere:\n\\(Y\\) is the outcome variable;\n\\(T\\) is a continuous variable which indicates the time passed from start of the observational period;\n\\(D\\) is a dummy variable indicating observation collected before (\\(D=0\\)) or after (\\(D=1\\)) the intervention;\n\\(P\\) is a continuous variable indicating time passed since the intervention has occurred (before intervention has occurred \\(P\\)is equal to \\(0\\));\nWith \\(\\epsilon\\) representing a zero centered gaussian random error.\n\nCounterfactual\n  \n\nWhat would have happened had Neo chosen the blue pill?\n\nIn an ITS it is important to understand the counterfactual. The counterfactual refers to what it would have occurred to Y, had the policy intervention not happened.\n\nüìùCounterfactuals are simply ways of comparing what happens given a change, versus what should have happened had some change not occurred in the first place.\n\nIn a randomized trial or A/B test we know the counterfactual average outcome because the experiment withheld the intervention from the control group (which by randomization is somewhat the same as the intervention group). A critical assumption in ITS is that the outcome of interest trend would remain unchanged in the absence of intervention."
  },
  {
    "objectID": "posts/2020_01_01_interrupted-time-series-python-part-I/index.html#a-practical-example",
    "href": "posts/2020_01_01_interrupted-time-series-python-part-I/index.html#a-practical-example",
    "title": "Interrupted Time Series (ITS) in Python",
    "section": "A practical example",
    "text": "A practical example\nBob runs a large and successful blog on personal finance. During a webinar he learns that making his web content load faster could reduce its bounce rate and therefore decides to sign up for a CDN service. It‚Äôs been 6 months since he added a CDN to his blog and he wants to know if the investiment he did reduced the bounce rate.\n\nDataset\nBob provides us with üíæ 24 weeks of data before adding the CDN and 24 weeks after it (intervention). Therefore, weeks 1 to 24 have a bouncing rate before intervention and weeks 25 to 48 after it.\n\n  \n\nVisually, it looks like after enabling the CDN the bounce rate decreased, but by how much, and does it have statistical significance? To answer this question using interrupted time series analysis, we first need to prepare our data.\n\n\nDataset preparation\nUsing equation (1) notation we üíæ enrich this data with values for columns \\(D\\) (\\(0\\) = before intervention, \\(1\\) after) and \\(P\\) (number of weeks since intervention started):\n\n\n\n\n\n\n\n\n\nBouncing rate(Y)\nWeek (T)\nIntervention(D)\nIntervention week(P)\n\n\n\n\n12.92\n1\n0\n0\n\n\n13.03\n2\n0\n0\n\n\n13.06\n3\n0\n0\n\n\n13.17\n4\n0\n0\n\n\n‚Ä¶\n‚Ä¶\n‚Ä¶\n‚Ä¶\n\n\n12.04\n45\n1\n21\n\n\n12.45\n46\n1\n22\n\n\n12.74\n47\n1\n23\n\n\n12.57\n48\n1\n24"
  },
  {
    "objectID": "posts/2020_01_01_interrupted-time-series-python-part-I/index.html#naive-solution",
    "href": "posts/2020_01_01_interrupted-time-series-python-part-I/index.html#naive-solution",
    "title": "Interrupted Time Series (ITS) in Python",
    "section": "Naive solution",
    "text": "Naive solution\nLet‚Äôs implement an ordinary least squares (OLS) regression using statsmodels to measure the impact of our intervention:\nimport pandas as pd\nimport statsmodels.api as sm\nimport statsmodels.formula.api as smf\n\ndf = pd.read_csv(\"enriched_data.csv\")\n\nmodel = smf.ols(formula='Y ~ T + D + P', data=df)\nres = model.fit()\nprint(res.summary())\nWith output:\n                            OLS Regression Results                            \n==============================================================================\nDep. Variable:                      Y   R-squared:                       0.666\nModel:                            OLS   Adj. R-squared:                  0.643\nMethod:                 Least Squares   F-statistic:                     29.18\nDate:                Tue, 28 Dec 2021   Prob (F-statistic):           1.52e-10\nTime:                        14:33:50   Log-Likelihood:                 4.8860\nNo. Observations:                  48   AIC:                            -1.772\nDf Residuals:                      44   BIC:                             5.713\nDf Model:                           3                                         \nCovariance Type:            nonrobust                                         \n==============================================================================\n                 coef    std err          t      P&gt;|t|      [0.025      0.975]\n------------------------------------------------------------------------------\nIntercept     12.9100      0.096    134.225      0.000      12.716      13.104\nT              0.0129      0.007      1.920      0.061      -0.001       0.026\nD             -0.5202      0.132     -3.942      0.000      -0.786      -0.254\nP             -0.0297      0.010     -3.115      0.003      -0.049      -0.010\n==============================================================================\nOmnibus:                        3.137   Durbin-Watson:                   0.665\nProb(Omnibus):                  0.208   Jarque-Bera (JB):                1.995\nSkew:                           0.279   Prob(JB):                        0.369\nKurtosis:                       2.172   Cond. No.                         125.\n==============================================================================\n\n\nThe model estimates that the bounce rate decreased üîª 0.52% and this effect is statistically significant (\\(P&gt;|t|\\) is virtually zero).\nIt is also noteworth that the model estimates a small (on average üîª 0.0297%) but with statistical significance trend of a decrease in bounce rate each week after intervention, which is unexpected since the CDN serves the whole website just a few hours after activation.\nThe figure below depicts how the model fits before and after intervention and how it project a counterfactual would be:\n\nstart = 24\nend = 48\nbeta = res.params\n\n# Get model predictions and 95% confidence interval\npredictions = res.get_prediction(df)\nsummary = predictions.summary_frame(alpha=0.05)\n\n# mean predictions\ny_pred = predictions.predicted_mean\n\n# countefactual assumes no interventions\ncf_df = df.copy()\ncf_df[\"D\"] = 0.0\ncf_df[\"P\"] = 0.0\n\n# counter-factual predictions\ncf = res.get_prediction(cf_df).summary_frame(alpha=0.05)\n\n# Plotting\nplt.style.use('seaborn-whitegrid')\nfig, ax = plt.subplots(figsize=(16,10))\n\n# Plot bounce rate data\nax.scatter(df[\"T\"], df[\"Y\"], facecolors='none', edgecolors='steelblue', label=\"bounce rate data\", linewidths=2)\n\n# Plot model mean bounce rate prediction\nax.plot(df[\"T\"][:start], y_pred[:start], 'b-', label=\"model prediction\")\nax.plot(df[\"T\"][start:], y_pred[start:], 'b-')\n\n# Plot counterfactual mean bounce rate with 95% confidence interval\nax.plot(df[\"T\"][start:], cf['mean'][start:], 'k.', label=\"counterfactual\")\nax.fill_between(df[\"T\"][start:], cf['mean_ci_lower'][start:], cf['mean_ci_upper'][start:], color='k', alpha=0.1, label=\"counterfactual 95% CI\");\n\n# Plot line marking intervention moment\nax.axvline(x = 24.5, color = 'r', label = 'intervention')\n\nax.legend(loc='best')\nplt.ylim([10, 15])\nplt.xlabel(\"Weeks\")\nplt.ylabel(\"Bounce rate (%)\");\n\n  \n\n\nProblems with naive approach\n\n  \n\nOLS (Ordinary Least Squares) regression has seven main assumptions but for brevity in this article we will focus on two only:\n\nIndividual observations are independent.\nResiduals follow a normal distribution.\n\n\nLet‚Äôs first check for the normality of residuals:\nWe can apply the Jarque-Bera test on residuals to checks whether their skewness and kurtosis match a normal distribution (\\(H_0\\): residual distribution follows a normal distribution). Our statsmodels OLS summary output shows a Prob(JB): 0.369 which for a standard \\(\\alpha\\) level of 0.05 doesn‚Äôt allow us discard null hypothesis (\\(H_0\\)).\nLet‚Äôs plot the distribution of residuals:\n    res.resid.plot(kind=\"kde\")\n\n  \n\nWhich for a small dataset (less than 50 points) looks sufficiently gaussian.\nOverall, the assumption of normality of residuals can‚Äôt be convincingly refuted. ‚úÖ\n\n\nChecking independence of observations:\nThe Durbin-Watson statistic test if the residuals are correlated with its immediate predecessor, that is, if they have an autocorrelation at lag 1 or \\(AR(1)\\). Its value ranges from 0 to 4 and values smaller than 1.5 indicate a positive autocorrelation, while values greater than 2.5 signal a negative autocorrelation.\nIf we take a look again at our OLS summary output we will observe that the Durbin-Watson statistic has a value of 0.665 which signals a strong positive \\(AR(1)\\).\nLet‚Äôs plot the residuals to see if we can observe this autocorrelation:\nimport altair as alt\n\nrules = alt.Chart(pd.DataFrame({\n  'residuals': [0.0],\n  'color': ['black']\n})).mark_rule().encode(\n  y='residuals',\n  color=alt.Color('color:N', scale=None)\n)\n\nresidual_plot = alt.Chart(res_df).mark_point().encode(\n    x=alt.X('Weeks'),\n    y=alt.Y('residuals')\n)\n\nrules + residual_plot \n\n  \n\nNotice how residuals above/below zero have most points temporally close to it also above/below zero as well, which goes against the independence of observations assumption of OLS ‚ùå.\n\nüìùIn practice when analyzing time series data the presence of autocorrelation is the rule instead of the exception since in general the factors that contributed to a given observation tend to persist for a while."
  },
  {
    "objectID": "posts/2020_01_01_interrupted-time-series-python-part-I/index.html#autoregressive-model-solution",
    "href": "posts/2020_01_01_interrupted-time-series-python-part-I/index.html#autoregressive-model-solution",
    "title": "Interrupted Time Series (ITS) in Python",
    "section": "Autoregressive model solution",
    "text": "Autoregressive model solution\nThe autoregressive model specifies that each observation depends linearly on previous observations.\nThus, an autoregressive model of order \\(p\\) (\\(AR(p)\\)) can be written as\n \\[y_t = c + \\phi_1 y_{t-1}+ \\dots + \\phi_p y_{t-p} + \\epsilon_t\\] \nWhere:\n\\(y_t\\): observation at time \\(t\\),\n\\(y_{t-i}\\): observation at time \\(t - i\\),\n\\(\\phi_i\\): coefficient of how much observation \\(y_{t - i}\\) correlates to \\(y_t\\),\n\\(\\epsilon_t\\): white noise ( \\(\\mathcal{N}(0, \\sigma¬≤)\\) ) at time \\(t\\).\n\nAutocorrelation\nTo assess how much an observation correlates with past observations it is useful to do an autocorrelation plot as shown below:\nsm.graphics.tsa.plot_acf(res.resid, lags=10)\nplt.show()\n\n  \n\n\n\nPartial Autocorrelation\nThe partial autocorrelation at lag \\(p\\) is the correlation that results after removing the effect of any correlations due to the terms at shorter lags.\nsm.graphics.tsa.plot_pacf(res.resid, lags=10)\nplt.show()   \n\n  \n\n\n\nModel selection\nThe theory states that in an autoregressive model its autocorrelation plot should depict an exponential decay and the number of lags \\(p\\) should be taken from the partial autocorrelation chart using its \\(p\\) most relevant lags. Applying the theory to our plots above, we conclude that our model is autoregressive of lag 1 also known as AR(1).\n\n\nARIMA\nIn statistics ARIMA stands for autoregressive integrated moving average model and as can be inferred by the name AR models are as especial case of ARIMA therefore AR(1) is equivalent to ARIMA(1,0,0).\nWe can model an AR(1) process to our dataset using statsmodels ARIMA as below:\nfrom statsmodels.tsa.arima.model import ARIMA\n\narima_results = ARIMA(df[\"Y\"], df[[\"T\",\"D\",\"P\"]], order=(1,0,0)).fit()\nprint(arima_results.summary())\nOutput:\n                               SARIMAX Results                                \n==============================================================================\nDep. Variable:                      Y   No. Observations:                   48\nModel:                 ARIMA(1, 0, 0)   Log Likelihood                  18.574\nDate:                Thu, 30 Dec 2021   AIC                            -25.148\nTime:                        01:51:46   BIC                            -13.921\nSample:                             0   HQIC                           -20.905\n                                 - 48                                         \nCovariance Type:                  opg                                         \n==============================================================================\n                 coef    std err          z      P&gt;|z|      [0.025      0.975]\n------------------------------------------------------------------------------\nconst         12.9172      0.279     46.245      0.000      12.370      13.465\nT              0.0121      0.016      0.767      0.443      -0.019       0.043\nD             -0.5510      0.273     -2.018      0.044      -1.086      -0.016\nP             -0.0238      0.021     -1.155      0.248      -0.064       0.017\nar.L1          0.6635      0.138      4.803      0.000       0.393       0.934\nsigma2         0.0267      0.006      4.771      0.000       0.016       0.038\n===================================================================================\nLjung-Box (L1) (Q):                   1.00   Jarque-Bera (JB):                 0.15\nProb(Q):                              0.32   Prob(JB):                         0.93\nHeteroskedasticity (H):               1.44   Skew:                            -0.05\nProb(H) (two-sided):                  0.47   Kurtosis:                         3.25\n===================================================================================\nThe autoregressive model estimates that the bounce rate decreased üîª 0.55% on average and this effect is statistically significant (\\(P&gt;|t| = 4.4\\%\\), less than our \\(\\alpha = 5\\%\\)).\nHowever, unlike the previous OLS model, the autoregressive model does not estimate a statistical significance trend of a decrease in bounce rate each week after intervention, which is in line with our expectations.\nThe models estimates (with counterfactual projections) can be seen in the chart below:\n\nfrom statsmodels.tsa.arima.model import ARIMA\n\nstart = 24\nend = 48\n\npredictions = arima_results.get_prediction(0, end-1)\nsummary = predictions.summary_frame(alpha=0.05)\n\narima_cf = ARIMA(df[\"Y\"][:start], df[\"T\"][:start], order=(1,0,0)).fit()\n\n# Model predictions means\ny_pred = predictions.predicted_mean\n\n# Counterfactual mean and 95% confidence interval\ny_cf = arima_cf.get_forecast(24, exog=df[\"T\"][start:]).summary_frame(alpha=0.05)\n\n# Plot section\nplt.style.use('seaborn-whitegrid')\nfig, ax = plt.subplots(figsize=(16,10))\n\n# Plot bounce rate data\nax.scatter(df[\"T\"], df[\"Y\"], facecolors='none', edgecolors='steelblue', label=\"bounce rate data\", linewidths=2)\n\n# Plot model mean bounce prediction\nax.plot(df[\"T\"][:start], y_pred[:start], 'b-', label=\"model prediction\")\nax.plot(df[\"T\"][start:], y_pred[start:], 'b-')\n\n# Plot counterfactual mean bounce rate with 95% confidence interval\nax.plot(df[\"T\"][start:], y_cf[\"mean\"], 'k.', label=\"counterfactual\")\nax.fill_between(df[\"T\"][start:], y_cf['mean_ci_lower'], y_cf['mean_ci_upper'], color='k', alpha=0.1, label=\"counterfactual 95% CI\");\n\n\n# Plot line marking intervention moment\nax.axvline(x = 24.5, color = 'r', label = 'intervention')\n\nax.legend(loc='best')\nplt.ylim([10, 15])\nplt.xlabel(\"Weeks\")\nplt.ylabel(\"Bounce rate (%)\");\n\n  \n\nWe can clearly see that the ARIMA(1, 0, 0) model fits our dataset better than the OLS model.\n\n\nARIMA residual analysis\nThe summary of our autoregressive model shows a Prob(JB): 0.93 which is compatible with the null-hypothesis of normaly distributed residuals. ‚úÖ\nThe Ljung-Box Q test verifies whether the residuals are independently distributed (they exhibit no serial autocorrelation) as \\(H_0\\) (null-hypothesis). As the Prob(Q): 0.32 is way above the standard \\(\\alpha = 0.05\\) there is no evidence of serial autocorrelation in the ARIMA residuals. ‚úÖ\nLet‚Äôs now take a look at residuals qqplot to check if they follow a normal distribution:\n\nimport scipy as sp\nfrom statsmodels.graphics.gofplots import qqplot\n\nfig, (ax1, ax2) = plt.subplots(1,2, figsize=(16,8))\nsm.qqplot(res.resid, sp.stats.t, fit=True, line=\"45\", ax=ax1);\nax1.set_title(\"OLS qqplot\");\n\nsm.qqplot(arima_results.resid, sp.stats.t, fit=True, line=\"45\", ax=ax2);\nax2.set_title(\"ARIMA qqplot\");\nplt.show();\n\n  \n\nWe may observe that the ARIMA(1,0,0) model residuals not only are in general normally distributed as they fit better than the OLS model the theoretical quantiles. ‚úÖ"
  },
  {
    "objectID": "posts/2020_01_01_interrupted-time-series-python-part-I/index.html#summary",
    "href": "posts/2020_01_01_interrupted-time-series-python-part-I/index.html#summary",
    "title": "Interrupted Time Series (ITS) in Python",
    "section": "Summary",
    "text": "Summary\nA/B tests are a the most powerful and trustworthy method to do measure the impact of modifications/changes even before they are fully implemented, which is why they are so widely used.\nHowever, there are some scenarios where A/B tests are not feasible and this is when the knowledge of quasi-experiments becomes valuable to get statistically sound measurements of change impact.\nIn this post we have shown why an ordinary least square (OLS) linear regression is not a good modeling approach for time series data since they usualy present non-negligible autocorrelation that violates some assumptions of OLS.\nWe demonstrated with an example how to use python (statsmodels, matplotlib, altair and pandas) to visualize residuals and plot autocorrelation and partial autocorrelations charts to figure out the lag of an autoregressive model and then implemented a ARIMA model using statsmodels to observed a more accurate and precise analysis and how to interpret statsmodels model output for OLS and ARIMA.\nWe also showed how to plot in a single chart the models estimates (mean and 95% confidence interval) for the time periods before and after intervention and its respective counterfactual."
  },
  {
    "objectID": "posts/2020_01_01_interrupted-time-series-python-part-I/index.html#references",
    "href": "posts/2020_01_01_interrupted-time-series-python-part-I/index.html#references",
    "title": "Interrupted Time Series (ITS) in Python",
    "section": "References",
    "text": "References\n[1] Shopify Engineering: How to Use Quasi-experiments and Counterfactuals to Build Great Products.\n[2] Wikipedia: Interrupted Time Series.\n[3] Campbell DT, Stanley JC. Experimental and Quasi-experimental Designs for Research. Boston, MA: Houghton Mifflin, 1963."
  },
  {
    "objectID": "posts/2023_07_20_using_pip_tools_to_manage_python_project_dependencies/index.html",
    "href": "posts/2023_07_20_using_pip_tools_to_manage_python_project_dependencies/index.html",
    "title": "Using pip-tools to manage project dependencies in Python",
    "section": "",
    "text": "As an engineer who loves to solve problems using Python and creates tens of projects by year, keeping track of different packages and their versions can be complex. I often have to test and deploy my projects in different environments (development/testing/production machines) and on different Cloud or PaaS providers and need to be sure that all of them will use exactly the same Python packages and versions as I have used in the development, to be sure no problems were introduced by an unexpected package upgrade.\nA naive approach would be to install the packages you need using pip and at the end generate a requirements.txt file with\npip freeze &gt; requirements.txt\nto persist all dependencies (with their versions) that got installed.We can later install into an empty virtual environment using that requirements file gets us the same packages and versions.\nBut this approch is problematic beause is virtually impossible to know which packages are needed by our application and which were pulled in as dependencies:\n\nsuppose I choose to switch from pandas to polars in my project. In that scenario, the requirements.txt file may have many pandas dependencies that are superfluous to polars. Unfortunately, we are uncertain of who they are.\nif you need to upgrade some of the packages (i.e Django version from 3.2 to 4.2), which Django dependencies need to be upgrated and to which version? Which new dependencies the new Django version have?\n\nThat is why I use the pip-tools project to simplify the process of package dependency management. In this blog post, we will discuss what pip-tools does, the problem it solves and why developers should consider using it, and provide five examples using pip-tools for managing Python project dependencies. Additionally, we will explore some alternatives to pip-tools."
  },
  {
    "objectID": "posts/2023_07_20_using_pip_tools_to_manage_python_project_dependencies/index.html#improve-your-python-dependency-management-with-pip-tools",
    "href": "posts/2023_07_20_using_pip_tools_to_manage_python_project_dependencies/index.html#improve-your-python-dependency-management-with-pip-tools",
    "title": "Using pip-tools to manage project dependencies in Python",
    "section": "",
    "text": "As an engineer who loves to solve problems using Python and creates tens of projects by year, keeping track of different packages and their versions can be complex. I often have to test and deploy my projects in different environments (development/testing/production machines) and on different Cloud or PaaS providers and need to be sure that all of them will use exactly the same Python packages and versions as I have used in the development, to be sure no problems were introduced by an unexpected package upgrade.\nA naive approach would be to install the packages you need using pip and at the end generate a requirements.txt file with\npip freeze &gt; requirements.txt\nto persist all dependencies (with their versions) that got installed.We can later install into an empty virtual environment using that requirements file gets us the same packages and versions.\nBut this approch is problematic beause is virtually impossible to know which packages are needed by our application and which were pulled in as dependencies:\n\nsuppose I choose to switch from pandas to polars in my project. In that scenario, the requirements.txt file may have many pandas dependencies that are superfluous to polars. Unfortunately, we are uncertain of who they are.\nif you need to upgrade some of the packages (i.e Django version from 3.2 to 4.2), which Django dependencies need to be upgrated and to which version? Which new dependencies the new Django version have?\n\nThat is why I use the pip-tools project to simplify the process of package dependency management. In this blog post, we will discuss what pip-tools does, the problem it solves and why developers should consider using it, and provide five examples using pip-tools for managing Python project dependencies. Additionally, we will explore some alternatives to pip-tools."
  },
  {
    "objectID": "posts/2023_07_20_using_pip_tools_to_manage_python_project_dependencies/index.html#what-is-pip-tools",
    "href": "posts/2023_07_20_using_pip_tools_to_manage_python_project_dependencies/index.html#what-is-pip-tools",
    "title": "Using pip-tools to manage project dependencies in Python",
    "section": "What is pip-tools?",
    "text": "What is pip-tools?\npip-tools is an open-source project created to simplify the requirements files management used in Python projects. It takes a project‚Äôs dependencies and recursively generates pinned version requirement files.\nMoreover, pip-tools allows you to easily manage your project dependencies with minimum effort and ensures you have a reproducible development environment. It allows other developers that are working on the same project to have identical dependencies to yours, with no version conflicts.\nThis can help reduce time spent debugging issues related to package conflicts and ensure that code runs consistently across different environments.\n\nIt streamlines the generation and maintenance of requirements files\nIt ensures that all packages are pinned to a specific version to avoid version conflicts\nIt provides an easier way to manage different environments, like production, development, and testing\nIt permits simple package updating and upgrading as new versions become available"
  },
  {
    "objectID": "posts/2023_07_20_using_pip_tools_to_manage_python_project_dependencies/index.html#example-of-using-pip-tools",
    "href": "posts/2023_07_20_using_pip_tools_to_manage_python_project_dependencies/index.html#example-of-using-pip-tools",
    "title": "Using pip-tools to manage project dependencies in Python",
    "section": "Example of using pip-tools",
    "text": "Example of using pip-tools\nHere are five examples of how developers can use ppip-tools to manage the package requirements of their Python projects:\n\nBasic usage of pip-tools with requirements files\nDevelopers can use pip-tools to manage their project dependencies by creating a requirements.in file with the required packages and their respective versions.\n\nListing¬†1: requirements.in\npandas&gt;=2.0\nscikit-learn\nxgboost==1.7.6\n\nDevelopers can then run the following command to generate the required requirements.txt file:\n$ pip-compile requirements.in\nThis generates a pinned version requirement file with all the packages and their respective dependencies:\n\nListing¬†2: Generated requirements.txt\n#\n# This file is autogenerated by pip-compile with Python 3.10\n# by the following command:\n#\n#    pip-compile requirements.in\n#\njoblib==1.3.1\n    # via scikit-learn\nnumpy==1.25.1\n    # via\n    #   pandas\n    #   scikit-learn\n    #   scipy\n    #   xgboost\npandas==2.0.3\n    # via -r requirements.in\npython-dateutil==2.8.2\n    # via pandas\npytz==2023.3\n    # via pandas\nscikit-learn==1.3.0\n    # via -r requirements.in\nscipy==1.11.1\n    # via\n    #   scikit-learn\n    #   xgboost\nsix==1.16.0\n    # via python-dateutil\nthreadpoolctl==3.2.0\n    # via scikit-learn\ntzdata==2023.3\n    # via pandas\nxgboost==1.7.6\n    # via -r requirements.in\n\n\n\npip-tools with multiple environments\nWe can define several files with libraries to use in multiple environments, such as development, production, and testing. We can create corresponding dev-requirements.in, prod-requirements.in, and test-requirements.in files and then use the following commands to generate their respective files:\n$ pip-compile dev-requirements.in\n$ pip-compile prod-requirements.in\n$ pip-compile test-requirements.in\nThese commands will generate dev-requirements.txt, prod-requirements.txt, and test-requirements.txt files with the corresponding dependencies.\n\n\npip-tools with custom package indexes\nWe can also use pip-tools with package indexes different from the official PyPI index. To do this, we can specify a custom index in their requirements.in file, like this:\n--index-url https://custompackageindex.com/\ndjango==3.2.5\nThen, running pip-compile requirements.in will output a requirements.txt file with the packages pinned to versions on the custom package index.\n\n\npip-tools with hash dependencies\nIf you want to pin your dependencies to an specific binary wheel compilation hash (and not only to a package-version) you can use following command:\n$ pip-compile --generate-hashes requirements.in\nThis will add a hash value to each package in the requirements file, including the transitive dependencies. This is usefull to increase security as PyPI has ‚Äúmade the deliberate choice to allow wheel files to be added to old releases‚Äù\n\n\nShow requirements.txt file with generated hashes\n\n\n# This file is autogenerated by pip-compile with Python 3.10\n# by the following command:\n#\n#    pip-compile --generate-hashes requirements.in\n#\njoblib==1.3.1 \\\n    --hash=sha256:1f937906df65329ba98013dc9692fe22a4c5e4a648112de500508b18a21b41e3 \\\n    --hash=sha256:89cf0529520e01b3de7ac7b74a8102c90d16d54c64b5dd98cafcd14307fdf915\n    # via scikit-learn\nnumpy==1.25.1 \\\n    --hash=sha256:012097b5b0d00a11070e8f2e261128c44157a8689f7dedcf35576e525893f4fe \\\n    --hash=sha256:0d3fe3dd0506a28493d82dc3cf254be8cd0d26f4008a417385cbf1ae95b54004 \\\n    --hash=sha256:0def91f8af6ec4bb94c370e38c575855bf1d0be8a8fbfba42ef9c073faf2cf19 \\\n    --hash=sha256:1a180429394f81c7933634ae49b37b472d343cccb5bb0c4a575ac8bbc433722f \\\n    --hash=sha256:1d5d3c68e443c90b38fdf8ef40e60e2538a27548b39b12b73132456847f4b631 \\\n    --hash=sha256:20e1266411120a4f16fad8efa8e0454d21d00b8c7cee5b5ccad7565d95eb42dd \\\n    --hash=sha256:247d3ffdd7775bdf191f848be8d49100495114c82c2bd134e8d5d075fb386a1c \\\n    --hash=sha256:35a9527c977b924042170a0887de727cd84ff179e478481404c5dc66b4170009 \\\n    --hash=sha256:38eb6548bb91c421261b4805dc44def9ca1a6eef6444ce35ad1669c0f1a3fc5d \\\n    --hash=sha256:3d7abcdd85aea3e6cdddb59af2350c7ab1ed764397f8eec97a038ad244d2d105 \\\n    --hash=sha256:41a56b70e8139884eccb2f733c2f7378af06c82304959e174f8e7370af112e09 \\\n    --hash=sha256:4a90725800caeaa160732d6b31f3f843ebd45d6b5f3eec9e8cc287e30f2805bf \\\n    --hash=sha256:6b82655dd8efeea69dbf85d00fca40013d7f503212bc5259056244961268b66e \\\n    --hash=sha256:6c6c9261d21e617c6dc5eacba35cb68ec36bb72adcff0dee63f8fbc899362588 \\\n    --hash=sha256:77d339465dff3eb33c701430bcb9c325b60354698340229e1dff97745e6b3efa \\\n    --hash=sha256:791f409064d0a69dd20579345d852c59822c6aa087f23b07b1b4e28ff5880fcb \\\n    --hash=sha256:9a3a9f3a61480cc086117b426a8bd86869c213fc4072e606f01c4e4b66eb92bf \\\n    --hash=sha256:c1516db588987450b85595586605742879e50dcce923e8973f79529651545b57 \\\n    --hash=sha256:c40571fe966393b212689aa17e32ed905924120737194b5d5c1b20b9ed0fb171 \\\n    --hash=sha256:d412c1697c3853c6fc3cb9751b4915859c7afe6a277c2bf00acf287d56c4e625 \\\n    --hash=sha256:d5154b1a25ec796b1aee12ac1b22f414f94752c5f94832f14d8d6c9ac40bcca6 \\\n    --hash=sha256:d736b75c3f2cb96843a5c7f8d8ccc414768d34b0a75f466c05f3a739b406f10b \\\n    --hash=sha256:e8f6049c4878cb16960fbbfb22105e49d13d752d4d8371b55110941fb3b17800 \\\n    --hash=sha256:f76aebc3358ade9eacf9bc2bb8ae589863a4f911611694103af05346637df1b7 \\\n    --hash=sha256:fd67b306320dcadea700a8f79b9e671e607f8696e98ec255915c0c6d6b818503\n    # via\n    #   pandas\n    #   scikit-learn\n    #   scipy\n    #   xgboost\npandas==2.0.3 \\\n    --hash=sha256:04dbdbaf2e4d46ca8da896e1805bc04eb85caa9a82e259e8eed00254d5e0c682 \\\n    --hash=sha256:1168574b036cd8b93abc746171c9b4f1b83467438a5e45909fed645cf8692dbc \\\n    --hash=sha256:1994c789bf12a7c5098277fb43836ce090f1073858c10f9220998ac74f37c69b \\\n    --hash=sha256:258d3624b3ae734490e4d63c430256e716f488c4fcb7c8e9bde2d3aa46c29089 \\\n    --hash=sha256:32fca2ee1b0d93dd71d979726b12b61faa06aeb93cf77468776287f41ff8fdc5 \\\n    --hash=sha256:37673e3bdf1551b95bf5d4ce372b37770f9529743d2498032439371fc7b7eb26 \\\n    --hash=sha256:3ef285093b4fe5058eefd756100a367f27029913760773c8bf1d2d8bebe5d210 \\\n    --hash=sha256:5247fb1ba347c1261cbbf0fcfba4a3121fbb4029d95d9ef4dc45406620b25c8b \\\n    --hash=sha256:5ec591c48e29226bcbb316e0c1e9423622bc7a4eaf1ef7c3c9fa1a3981f89641 \\\n    --hash=sha256:694888a81198786f0e164ee3a581df7d505024fbb1f15202fc7db88a71d84ebd \\\n    --hash=sha256:69d7f3884c95da3a31ef82b7618af5710dba95bb885ffab339aad925c3e8ce78 \\\n    --hash=sha256:6a21ab5c89dcbd57f78d0ae16630b090eec626360085a4148693def5452d8a6b \\\n    --hash=sha256:81af086f4543c9d8bb128328b5d32e9986e0c84d3ee673a2ac6fb57fd14f755e \\\n    --hash=sha256:9e4da0d45e7f34c069fe4d522359df7d23badf83abc1d1cef398895822d11061 \\\n    --hash=sha256:9eae3dc34fa1aa7772dd3fc60270d13ced7346fcbcfee017d3132ec625e23bb0 \\\n    --hash=sha256:9ee1a69328d5c36c98d8e74db06f4ad518a1840e8ccb94a4ba86920986bb617e \\\n    --hash=sha256:b084b91d8d66ab19f5bb3256cbd5ea661848338301940e17f4492b2ce0801fe8 \\\n    --hash=sha256:b9cb1e14fdb546396b7e1b923ffaeeac24e4cedd14266c3497216dd4448e4f2d \\\n    --hash=sha256:ba619e410a21d8c387a1ea6e8a0e49bb42216474436245718d7f2e88a2f8d7c0 \\\n    --hash=sha256:c02f372a88e0d17f36d3093a644c73cfc1788e876a7c4bcb4020a77512e2043c \\\n    --hash=sha256:ce0c6f76a0f1ba361551f3e6dceaff06bde7514a374aa43e33b588ec10420183 \\\n    --hash=sha256:d9cd88488cceb7635aebb84809d087468eb33551097d600c6dad13602029c2df \\\n    --hash=sha256:e4c7c9f27a4185304c7caf96dc7d91bc60bc162221152de697c98eb0b2648dd8 \\\n    --hash=sha256:f167beed68918d62bffb6ec64f2e1d8a7d297a038f86d4aed056b9493fca407f \\\n    --hash=sha256:f3421a7afb1a43f7e38e82e844e2bca9a6d793d66c1a7f9f0ff39a795bbc5e02\n    # via -r requirements.in\npython-dateutil==2.8.2 \\\n    --hash=sha256:0123cacc1627ae19ddf3c27a5de5bd67ee4586fbdd6440d9748f8abb483d3e86 \\\n    --hash=sha256:961d03dc3453ebbc59dbdea9e4e11c5651520a876d0f4db161e8674aae935da9\n    # via pandas\npytz==2023.3 \\\n    --hash=sha256:1d8ce29db189191fb55338ee6d0387d82ab59f3d00eac103412d64e0ebd0c588 \\\n    --hash=sha256:a151b3abb88eda1d4e34a9814df37de2a80e301e68ba0fd856fb9b46bfbbbffb\n    # via pandas\nscikit-learn==1.3.0 \\\n    --hash=sha256:0e8102d5036e28d08ab47166b48c8d5e5810704daecf3a476a4282d562be9a28 \\\n    --hash=sha256:151ac2bf65ccf363664a689b8beafc9e6aae36263db114b4ca06fbbbf827444a \\\n    --hash=sha256:1d54fb9e6038284548072df22fd34777e434153f7ffac72c8596f2d6987110dd \\\n    --hash=sha256:3a11936adbc379a6061ea32fa03338d4ca7248d86dd507c81e13af428a5bc1db \\\n    --hash=sha256:436aaaae2c916ad16631142488e4c82f4296af2404f480e031d866863425d2a2 \\\n    --hash=sha256:552fd1b6ee22900cf1780d7386a554bb96949e9a359999177cf30211e6b20df6 \\\n    --hash=sha256:6a885a9edc9c0a341cab27ec4f8a6c58b35f3d449c9d2503a6fd23e06bbd4f6a \\\n    --hash=sha256:7617164951c422747e7c32be4afa15d75ad8044f42e7d70d3e2e0429a50e6718 \\\n    --hash=sha256:79970a6d759eb00a62266a31e2637d07d2d28446fca8079cf9afa7c07b0427f8 \\\n    --hash=sha256:850a00b559e636b23901aabbe79b73dc604b4e4248ba9e2d6e72f95063765603 \\\n    --hash=sha256:8be549886f5eda46436b6e555b0e4873b4f10aa21c07df45c4bc1735afbccd7a \\\n    --hash=sha256:981287869e576d42c682cf7ca96af0c6ac544ed9316328fd0d9292795c742cf5 \\\n    --hash=sha256:9877af9c6d1b15486e18a94101b742e9d0d2f343d35a634e337411ddb57783f3 \\\n    --hash=sha256:998d38fcec96584deee1e79cd127469b3ad6fefd1ea6c2dfc54e8db367eb396b \\\n    --hash=sha256:9d953531f5d9f00c90c34fa3b7d7cfb43ecff4c605dac9e4255a20b114a27369 \\\n    --hash=sha256:ae80c08834a473d08a204d966982a62e11c976228d306a2648c575e3ead12111 \\\n    --hash=sha256:c470f53cea065ff3d588050955c492793bb50c19a92923490d18fcb637f6383a \\\n    --hash=sha256:c7e28d8fa47a0b30ae1bd7a079519dd852764e31708a7804da6cb6f8b36e3630 \\\n    --hash=sha256:ded35e810438a527e17623ac6deae3b360134345b7c598175ab7741720d7ffa7 \\\n    --hash=sha256:ee04835fb016e8062ee9fe9074aef9b82e430504e420bff51e3e5fffe72750ca \\\n    --hash=sha256:fd6e2d7389542eae01077a1ee0318c4fec20c66c957f45c7aac0c6eb0fe3c612\n    # via -r requirements.in\nscipy==1.11.1 \\\n    --hash=sha256:08d957ca82d3535b3b9ba6c8ff355d78fe975271874e2af267cb5add5bd78625 \\\n    --hash=sha256:249cfa465c379c9bb2c20123001e151ff5e29b351cbb7f9c91587260602c58d0 \\\n    --hash=sha256:366a6a937110d80dca4f63b3f5b00cc89d36f678b2d124a01067b154e692bab1 \\\n    --hash=sha256:39154437654260a52871dfde852adf1b93b1d1bc5dc0ffa70068f16ec0be2624 \\\n    --hash=sha256:396fae3f8c12ad14c5f3eb40499fd06a6fef8393a6baa352a652ecd51e74e029 \\\n    --hash=sha256:3b9963798df1d8a52db41a6fc0e6fa65b1c60e85d73da27ae8bb754de4792481 \\\n    --hash=sha256:3e8eb42db36526b130dfbc417609498a6192381abc1975b91e3eb238e0b41c1a \\\n    --hash=sha256:512fdc18c65f76dadaca139348e525646d440220d8d05f6d21965b8d4466bccd \\\n    --hash=sha256:aec8c62fbe52914f9cf28d846cf0401dd80ab80788bbab909434eb336ed07c04 \\\n    --hash=sha256:b41a0f322b4eb51b078cb3441e950ad661ede490c3aca66edef66f4b37ab1877 \\\n    --hash=sha256:b4bb943010203465ac81efa392e4645265077b4d9e99b66cf3ed33ae12254173 \\\n    --hash=sha256:b588311875c58d1acd4ef17c983b9f1ab5391755a47c3d70b6bd503a45bfaf71 \\\n    --hash=sha256:ba94eeef3c9caa4cea7b402a35bb02a5714ee1ee77eb98aca1eed4543beb0f4c \\\n    --hash=sha256:be8c962a821957fdde8c4044efdab7a140c13294997a407eaee777acf63cbf0c \\\n    --hash=sha256:cce154372f0ebe88556ed06d7b196e9c2e0c13080ecb58d0f35062dc7cc28b47 \\\n    --hash=sha256:d51565560565a0307ed06fa0ec4c6f21ff094947d4844d6068ed04400c72d0c3 \\\n    --hash=sha256:e866514bc2d660608447b6ba95c8900d591f2865c07cca0aa4f7ff3c4ca70f30 \\\n    --hash=sha256:fb5b492fa035334fd249f0973cc79ecad8b09c604b42a127a677b45a9a3d4289 \\\n    --hash=sha256:ffb28e3fa31b9c376d0fb1f74c1f13911c8c154a760312fbee87a21eb21efe31\n    # via\n    #   scikit-learn\n    #   xgboost\nsix==1.16.0 \\\n    --hash=sha256:1e61c37477a1626458e36f7b1d82aa5c9b094fa4802892072e49de9c60c4c926 \\\n    --hash=sha256:8abb2f1d86890a2dfb989f9a77cfcfd3e47c2a354b01111771326f8aa26e0254\n    # via python-dateutil\nthreadpoolctl==3.2.0 \\\n    --hash=sha256:2b7818516e423bdaebb97c723f86a7c6b0a83d3f3b0970328d66f4d9104dc032 \\\n    --hash=sha256:c96a0ba3bdddeaca37dc4cc7344aafad41cdb8c313f74fdfe387a867bba93355\n    # via scikit-learn\ntzdata==2023.3 \\\n    --hash=sha256:11ef1e08e54acb0d4f95bdb1be05da659673de4acbd21bf9c69e94cc5e907a3a \\\n    --hash=sha256:7e65763eef3120314099b6939b5546db7adce1e7d6f2e179e3df563c70511eda\n    # via pandas\nxgboost==1.7.6 \\\n    --hash=sha256:127cf1f5e2ec25cd41429394c6719b87af1456ce583e89f0bffd35d02ad18bcb \\\n    --hash=sha256:1c527554a400445e0c38186039ba1a00425dcdb4e40b37eed0e74cb39a159c47 \\\n    --hash=sha256:281c3c6f4fbed2d36bf95cd02a641afa95e72e9abde70064056da5e76233e8df \\\n    --hash=sha256:4c34675b4d2678c624ddde5d45361e7e16046923e362e4e609b88353e6b87124 \\\n    --hash=sha256:59b4b366d2cafc7f645e87d897983a5b59be02876194b1d213bd8d8b811d8ce8 \\\n    --hash=sha256:b1d5db49b199152d62bd9217c98760207d3de86d2b9d243260c573ffe638f80a\n    # via -r requirements.in\n\n\n\n\npip-tools to update dependencies\nFinally, pip-tools can also help developers manage package updates easily. They can run the following command to generate a requirements.txt file with the new versions of the packages:\n$ pip-compile --upgrade requirements.in \nThis command will identify the available updates and upgrade the packages listed in the requirements file."
  },
  {
    "objectID": "posts/2023_07_20_using_pip_tools_to_manage_python_project_dependencies/index.html#pip-sync",
    "href": "posts/2023_07_20_using_pip_tools_to_manage_python_project_dependencies/index.html#pip-sync",
    "title": "Using pip-tools to manage project dependencies in Python",
    "section": "pip-sync",
    "text": "pip-sync\npip-sync is a tool provided by pip-tools that ensures your virtual environment only contains the packages you have explicitly listed in your requirements.txt file. This is useful because it prevents conflicts between versions of packages in your virtual environment and ensures you only install the packages necessary for your project removing any previously installed packages that are not needed anymore.\nUsing pip-sync is a good practice to ensure your project dependencies are isolated from other projects on your development machine. It prevents conflicts between different versions of packages, making your project more reliable and robust.\nOnce you have your requirements.txt file, run the command:\n$ pip-sync\nThis command will install only the packages listed in requirements.txt and their dependencies, and remove any packages that are not listed. This ensures that you have an isolated environment with only the packages required by your project.\n\nExample: replacing pandas with polars\nIf we decide to move from pandas to polars we update our requirement.in file:\n\nListing¬†3: requirements.in after replacing pandas with polars\npolars\nscikit-learn\nxgboost==1.7.6\n\nand then run:\npip-compile\npip-sync\nwe will observe pip-sinc doing its magic:\nFound existing installation: pandas 2.0.3\nUninstalling pandas-2.0.3:\n  Successfully uninstalled pandas-2.0.3\nFound existing installation: python-dateutil 2.8.2\nUninstalling python-dateutil-2.8.2:\n  Successfully uninstalled python-dateutil-2.8.2\nFound existing installation: pytz 2023.3\nUninstalling pytz-2023.3:\n  Successfully uninstalled pytz-2023.3\nFound existing installation: six 1.16.0\nUninstalling six-1.16.0:\n  Successfully uninstalled six-1.16.0\nFound existing installation: tzdata 2023.3\nUninstalling tzdata-2023.3:\n  Successfully uninstalled tzdata-2023.3\nCollecting polars==0.18.9 (from -r /tmp/tmpwq_lrber (line 1))\n  Obtaining dependency information for polars==0.18.9 from https://files.pythonhosted.org/packages/32/b7/bb1faf1741235f1147408322d1cd845e29d195e2e4dc636a3dea8b4ea119/polars-0.18.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata\n  Using cached polars-0.18.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (14 kB)\nUsing cached polars-0.18.9-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (19.3 MB)\nInstalling collected packages: polars\nSuccessfully installed polars-0.18.9\nNotice how it removed all packages required only by pandas (see Listing¬†2) and then installed polars."
  },
  {
    "objectID": "posts/2023_07_20_using_pip_tools_to_manage_python_project_dependencies/index.html#alternatives-to-pip-tools",
    "href": "posts/2023_07_20_using_pip_tools_to_manage_python_project_dependencies/index.html#alternatives-to-pip-tools",
    "title": "Using pip-tools to manage project dependencies in Python",
    "section": "Alternatives to pip-tools",
    "text": "Alternatives to pip-tools\nWhile pip-tools is a popular and powerful tool for managing dependencies, developers can also use alternatives such as Poetry, Pipenv, conda, and setuptools. These tools all have slightly different approaches to the problem of managing package dependencies in Python projects but pip-tools has the advantage of being simpler and more lightweight while for instance poetry is preferable if you also want a more complete set of features for managing not only package dependencies, but also virtual environments, package building, and publishing."
  },
  {
    "objectID": "posts/2023_07_20_using_pip_tools_to_manage_python_project_dependencies/index.html#conclusion",
    "href": "posts/2023_07_20_using_pip_tools_to_manage_python_project_dependencies/index.html#conclusion",
    "title": "Using pip-tools to manage project dependencies in Python",
    "section": "Conclusion",
    "text": "Conclusion\npip-tools provides developers with an elegant and straightforward way to manage package dependencies in their Python projects. With its advantages such as generating pinned versions, managing multiple environments, and identifying available package updates, it simplifies the work of maintaining Python projects. By using pip-tools with different commands like pip-compile and pip-sync, developers can effectively solve the problem of managing their project dependencies."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "About",
    "section": "",
    "text": "Hey üëã, I am Flavio Regis de Arruda.\nI am a backend and data engineer who resides in Brazil üáßüá∑. With over 20 years of experience in backend and data science, I have cultivated a great passion for managing, manipulating and analyzing data in order to gain better insights and ultimately improve the world.\nIn my free time, I enjoy spending time with my two children, playing chess, traveling and watching movies. As I believe in the importance of maintaining a balance between work and personal life, I make sure to incorporate daily exercise into my routine. Additionally, I like to walk in nature at least once a week to recharge my energies."
  }
]